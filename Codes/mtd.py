import argparse
import numpy as np
import random
import networkx as nx
from copy import deepcopy
import sys
import os
import pickle
sys.path.append(os.getcwd())

from greedy_all import greedy
from epidemic_attack import e_attack, SV
from adapter import Agent
from drl import drl_train

def parse_args():
    parser = argparse.ArgumentParser("Reinforcement Learning experiments for multiagent environments")
    parser.add_argument('--train_times', metavar='N', type=int, default=100,
                    help='DRL training times/epoches')
    parser.add_argument('--attack_times', metavar='N', type=int, default=500,
                    help='attack simulation times')
    parser.add_argument('--l', metavar='N', type=int, default=5,
                    help='number of software versions')
    parser.add_argument('--lmd', metavar='%', type=float, default=2,
                    help='exponential(poisson) distribution parameter')
    parser.add_argument('--b', metavar='%', type=int, default=800,
                    help='budget')
    parser.add_argument('--p_s', metavar='%', type=float, default=0.3,
                    help='state_manupulation_attack')
    parser.add_argument('--d_r', metavar='%', type=float, default=0.8,
                    help='detection rate for state_manupulation_attack')
    parser.add_argument("--model", type=str, default="ppo", help="DRL model")
    parser.add_argument("--mode", type=str, default="time", help="MTD mode")
    parser.add_argument("--greedy", metavar='%', type=int, default=1)
    parser.add_argument("--drl", metavar='%', type=int, default=1)
    parser.add_argument("--rn", metavar='N', type=int, default=0)
    parser.add_argument("--part", metavar='N', type=int, default=0)
    return parser.parse_args()

class mtd(object):
    def __init__(self, g, attack_real, attack_model, arglist, evals = None, area = None):
        self.g = g
        self.length = arglist.b
        self.attack_model = attack_model#for simulation
        self.attack = attack_real
        self.agent = Agent(self.g, self.attack_model, a_times=arglist.attack_times)
        self.method = arglist.model
        if arglist.drl==1:
            self.drl_train = drl_train(self.agent, arglist, steps=1, episodes = 250, method = self.method, evals = evals)
        self.mode = arglist.mode
        self.initial_batch_list = [150]*5
        self.arglist = arglist
        self.area = area
        
    def reset(self,g):    
        self.g = g
        self.agent = Agent(self.g, self.attack_model, a_times=500)
        if self.arglist.drl==1:
            self.drl_train = drl_train(self.agent, self.arglist, steps=1, episodes = 250, method = self.method)
    
    def run(self, drl = True, seed = 1, round = 0):
        temp_length = min(nx.number_of_edges(self.g),self.length)
        if temp_length == 0:
            return deepcopy(self.g)
        if drl:
            if round == 0 :
                hot_area = self.area
            else:
                hot_area = greedy(self.agent, length = temp_length, step=20)
            #hot_area = [0, 64, 0, 0, 64, 0]
            print(hot_area)
            if hot_area:
                self.drl_train.set_area(hot_area)
                return self.drl_train.main(initial_batch_list=self.initial_batch_list,random_seed=seed)
            else:
                return deepcopy(self.g)
        else:
            temp_g = greedy(self.agent, length = temp_length, step=20,get_graph=True)
            if temp_g:
                return temp_g
            else:
                return deepcopy(self.g)
        
    def main(self, rounds=1, seed=1):
        records = []
        for i in range(rounds):
            if self.arglist.drl==1 and i % 4 == 0:
                g = self.run(drl = True, seed = seed, round = i)
            elif self.arglist.greedy==1:
                g = self.run(drl = False)
            record, g = self.attack.eval_single(1, g, mode=self.mode, record=True, seed=seed)
            records.append(record)
            self.reset(g)
        return records
    
if __name__ == "__main__":
    arglist = parse_args()
    attack_real = e_attack(idp=0.9, SV=SV(1,arglist.l), l=arglist.l, lmd = arglist.lmd, ac = 5)
    attack_model = e_attack(idp=0.9, SV=SV(1,arglist.l), l=arglist.l, lmd = arglist.lmd, ac = 20)
    g = nx.read_gpickle("graphs/rn_%d.gpickle"%(arglist.rn))
    #g = nx.read_gpickle("graphs/er_%d.gpickle"%(arglist.l))
    drl_evals = pickle.load(open("evals/eval_%d_%d_%d.pkl"%(arglist.lmd,arglist.rn,arglist.b),"rb"))
    drl_area = pickle.load(open("areas/area_%d_%d_%d.pkl"%(arglist.lmd,arglist.rn,arglist.b),"rb"))    
    #drl_evals = pickle.load(open("evals/eval_%d_%d_%d.pkl"%(arglist.lmd,arglist.l,arglist.b),"rb"))
    #drl_area = pickle.load(open("areas/area_%d_%d_%d.pkl"%(arglist.lmd,arglist.l,arglist.b),"rb"))
    records_list = []
    if arglist.part == 0:
        start = 1
        end = arglist.train_times+1
    elif arglist.part == 1:
        start = 1
        end = int(arglist.train_times/2)+1
    elif arglist.part == 2:
        start = int(arglist.train_times/2)+1
        end = arglist.train_times+1
    for i in range(start,end):
        print("Simulation: %d/%d"%(i,arglist.train_times))
        mtd_agent = mtd(deepcopy(g), deepcopy(attack_real), deepcopy(attack_model), arglist, evals = drl_evals, area = drl_area)
        records = mtd_agent.main(rounds=12,seed=i)
        records_list.append(records)
        with open("results/%s_%d_%d_%d_%d_%.2f_%.2f.pkl"%(arglist.model,arglist.greedy,arglist.part,arglist.rn,arglist.b,arglist.p_s,arglist.d_r), "wb") as fp:
            pickle.dump(records_list, fp)