from copy import deepcopy
from anytree import PreOrderIter
from drl_env import ENV, node, actions_to_str
from dqn import DQNAgent
from ppo import PPOAgent
import numpy as np
import torch
import time
from statistics import mean
from multiprocessing import Pool

class drl_train(object):
    def __init__(self, agent, args, steps=1, episodes = 2000, method = 'dqn', evals = None):
        self.agent = agent
        self.steps = steps
        self.episodes = episodes
        if method == 'dqn':
            self.method = DQNAgent
        elif method == 'ppo':
            self.method = PPOAgent
        self.temp = 0
        self.node_list = []
        self.args = args
        self.evals = evals
        
        
    def set_area(self, area):
        self.area = area
        self.env = ENV(self.agent, area=self.area)
        
    def get_all_evaluations(self, env,agent):
        budgets_list = list()
        budgets_eval = set()
        while len(budgets_eval)<25 and len(budgets_list)<50:
            state = deepcopy(env.reset())
            done = False
            while done == False:                
                current_action = agent.act(np.expand_dims(state, axis=0))
                state, r, done, center = env.skip_step(current_action,0,self.args)
                agent.buffer.is_terminals.append(done)
                agent.buffer.rewards.append(r)
                agent.buffer.next_states.append(state)
                agent.buffer.centers.append(center)
            if r == 0:
                budgets_list.append((env.centers[-1][0],env.centers[-1][1]))
                budgets_eval.add((env.centers[-1][0],env.centers[-1][1]))
        budgets_eval = list(set(budgets_list))
        budgets_indices = [budgets_eval.index(i) for i in budgets_list]
        return budgets_eval, budgets_indices
    
    def multi_evaluate(self, agent, input_array):
        pool = Pool()
        #print("Separated results:")
        input_array = [[i] for i in input_array]
        results = pool.starmap(agent.eval_all, input_array)
        pool.close()
        pool.join()
        return results
    
    def multi_preval(self, budgets_eval):
        results = []
        for budget in budgets_eval:
            x, y = budget
            #print(x,y)
            results.append(self.evals[x][y])
        return results
    
    def main(self,batch_size=0,random_seed=0,initial_batch_list = [],load=False,save=False,device = torch.device('cuda:0')):
        start = time.time()
        state_size = len(self.env.state)
        action_size = len(self.env.action_space)
        eval_set1, eval_set2 = set(), set()
        if batch_size > 0:
            batch_list = initial_batch_list+[batch_size]
        else:
            batch_list = initial_batch_list
        max_training_timesteps = sum(initial_batch_list)+int(batch_size-1)
        if random_seed:
            torch.manual_seed(random_seed)
            np.random.seed(random_seed)
        agent = self.method(state_size, action_size, device = device)
        reward_list = []
        max_reward = -np.inf
        episodes = 0
        for time_step in range(10):
            budgets_eval, budgets_indices = self.get_all_evaluations(self.env,agent)
            if self.evals:
                rewards = self.multi_preval(budgets_eval)
            else:
                rewards = self.multi_evaluate(self.agent, budgets_eval)
            episodes += len(budgets_indices)
            idx = np.argmax(np.array(rewards))
            if rewards[idx] > max_reward:
                max_reward = rewards[idx]
                budget = round(budgets_eval[idx][0]), round(budgets_eval[idx][1])
            episode = 0
            for idx in range(len(agent.buffer.is_terminals)):
                if agent.buffer.is_terminals[idx] == True and agent.buffer.rewards[idx] == 0:
                    #print(rewards,len(rewards),budgets_indices,episode)
                    temp_reward = rewards[budgets_indices[episode]]
                    agent.buffer.rewards[idx] = temp_reward
                    x,y = agent.buffer.centers[idx]
                    self.env.indicator[x,y] = temp_reward
                    episode += 1
            agent.replay()
            end = time.time()
            print("episode: %d, score: %.5f, eval times: %d, time cost: %.3f"
                  %(episodes, mean(rewards), len(budgets_eval), end-start))
        return self.agent.eval_all(budget=budget,get_graph=True)#self.agent.eval_all(budget=budget,get_graph=True)
